{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EMNIST.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN8rbchPc2BnqI2HA7CxiVO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tony1swedberg/EMNIST_CNN/blob/main/EMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hDTtfJXe2Mw"
      },
      "source": [
        "# Trains a convolutional neural network on the EMNIST ByClass dataset. The EMNIST ByClass dataset is a set of 814,255 handwritten characters and digits split into 62 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0McXaTLdJ72"
      },
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets         # For EMNIST dataset\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqX_Oy8CQRWo"
      },
      "source": [
        "epoch_num = 5             # Lower for training purposes\n",
        "batch_size = 100          #\n",
        "learning_rate = .01       #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMNDHJVhVEH_",
        "outputId": "65dcc1ac-1e25-4b48-e562-cac8524af17b"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    print('We are utilizing the GPU')\n",
        "else:\n",
        "    print('We are using the CPU')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We are utilizing the GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-_6S3odB3Bg"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5), (0.5))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldAAxWccdavA"
      },
      "source": [
        "#split can be byclass, bymerge, balanced, letters, digits, and mnist\n",
        "trainset = datasets.EMNIST(root='./data', train=True, download=True, split='byclass', transform=transform)\n",
        "testset = datasets.EMNIST(root='./data', train=False, download=True, split='byclass', transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VNmgEYD78aY"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5VfHW7t8z6_"
      },
      "source": [
        "#This cell is defining the structure of each CNN, NetOne is larger and takes much longer\n",
        "#to run than NetTwo. We are using NetTwo currently\n",
        "\n",
        "class NetOne(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.conv1 = nn.Conv2d(1, 50, 5)\n",
        "      self.pool = nn.MaxPool2d(2, 2)\n",
        "      self.conv2 = nn.Conv2d(50, 500, 3)\n",
        "      self.conv3 = nn.Conv2d(500, 10000, 2)\n",
        "      self.fc1 = nn.Linear(10000*2*2, 1000)\n",
        "      self.fc2 = nn.Linear(1000, 500)\n",
        "      self.fc3 = nn.Linear(500, 5000)\n",
        "      self.fc4 = nn.Linear(5000, 62)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = F.relu(self.conv1(x))\n",
        "      x = self.pool(x)\n",
        "      x = F.relu(self.conv2(x))\n",
        "      x = self.pool(x)\n",
        "      x = F.relu(self.conv3(x))\n",
        "      x = self.pool(x)\n",
        "      x = torch.flatten(x, 1)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = F.relu(self.fc3(x))\n",
        "      x = self.fc4(x)\n",
        "      return x\n",
        "\n",
        "class NetTwo(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.conv1 = nn.Conv2d(1, 10, 5)\n",
        "      self.pool = nn.MaxPool2d(2, 2)\n",
        "      self.conv2 = nn.Conv2d(10, 25, 3)\n",
        "      self.conv3 = nn.Conv2d(25, 50, 2)\n",
        "      self.fc1 = nn.Linear(50*2*2, 100)\n",
        "      self.fc2 = nn.Linear(100, 75)\n",
        "      self.fc3 = nn.Linear(75, 150)\n",
        "      self.fc4 = nn.Linear(150, 62)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = F.relu(self.conv1(x))\n",
        "      x = self.pool(x)\n",
        "      x = F.relu(self.conv2(x))\n",
        "      x = self.pool(x)\n",
        "      x = F.relu(self.conv3(x))\n",
        "      x = self.pool(x)\n",
        "      x = torch.flatten(x, 1)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = F.relu(self.fc3(x))\n",
        "      x = self.fc4(x)\n",
        "      return x\n",
        "\n",
        "net = NetTwo()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5PK4bwVRxsC"
      },
      "source": [
        "#This cell defines an accuracy function for our CNN\n",
        "\n",
        "def accuracy(net, loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in loader:\n",
        "        if torch.cuda.is_available():\n",
        "            net = net.cuda()\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Dy1F_smMDg0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d0d946f-9ca6-48a1-acce-f9ce31f63809"
      },
      "source": [
        "#This cell is where we do the training of the CNN\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=.9)\n",
        "for epoch in range(1, epoch_num + 1):\n",
        "    batch_num = 1\n",
        "    for inputs, labels in train_loader:\n",
        "        if torch.cuda.is_available():          #If the GPU is available we use it to speed up training\n",
        "            net = net.cuda()\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_num % 500 == 0:               #To see how quickly the model is training we print every 500 batches \n",
        "            print(f'We are on epoch {epoch} of {epoch_num}, batch {batch_num} of {len(train_loader)}')\n",
        "        batch_num += 1\n",
        "    train_accuracy = 100 * accuracy(net, train_loader)   #Calculate the train accuracy after each epoch\n",
        "    test_accuracy = 100 * accuracy(net, test_loader)     #Calculate the test accuracy after each epoch\n",
        "    print(f'For epoch {epoch} train accuracy is {train_accuracy}%, and test accuracy is {test_accuracy}%')\n",
        "print(f'The final test accuracy of this convolutional neural network on EMNIST is {test_accuracy}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We are on epoch 1 of 5, batch 500 of 6980\n",
            "We are on epoch 1 of 5, batch 1000 of 6980\n",
            "We are on epoch 1 of 5, batch 1500 of 6980\n",
            "We are on epoch 1 of 5, batch 2000 of 6980\n",
            "We are on epoch 1 of 5, batch 2500 of 6980\n",
            "We are on epoch 1 of 5, batch 3000 of 6980\n",
            "We are on epoch 1 of 5, batch 3500 of 6980\n",
            "We are on epoch 1 of 5, batch 4000 of 6980\n",
            "We are on epoch 1 of 5, batch 4500 of 6980\n",
            "We are on epoch 1 of 5, batch 5000 of 6980\n",
            "We are on epoch 1 of 5, batch 5500 of 6980\n",
            "We are on epoch 1 of 5, batch 6000 of 6980\n",
            "We are on epoch 1 of 5, batch 6500 of 6980\n",
            "For epoch 1 train accuracy is 84.4196855854152%, and test accuracy is 84.31350635729822%\n",
            "We are on epoch 2 of 5, batch 500 of 6980\n",
            "We are on epoch 2 of 5, batch 1000 of 6980\n",
            "We are on epoch 2 of 5, batch 1500 of 6980\n",
            "We are on epoch 2 of 5, batch 2000 of 6980\n",
            "We are on epoch 2 of 5, batch 2500 of 6980\n",
            "We are on epoch 2 of 5, batch 3000 of 6980\n",
            "We are on epoch 2 of 5, batch 3500 of 6980\n",
            "We are on epoch 2 of 5, batch 4000 of 6980\n",
            "We are on epoch 2 of 5, batch 4500 of 6980\n",
            "We are on epoch 2 of 5, batch 5000 of 6980\n",
            "We are on epoch 2 of 5, batch 5500 of 6980\n",
            "We are on epoch 2 of 5, batch 6000 of 6980\n",
            "We are on epoch 2 of 5, batch 6500 of 6980\n",
            "For epoch 2 train accuracy is 85.36175444026065%, and test accuracy is 85.12160105911987%\n",
            "We are on epoch 3 of 5, batch 500 of 6980\n",
            "We are on epoch 3 of 5, batch 1000 of 6980\n",
            "We are on epoch 3 of 5, batch 1500 of 6980\n",
            "We are on epoch 3 of 5, batch 2000 of 6980\n",
            "We are on epoch 3 of 5, batch 2500 of 6980\n",
            "We are on epoch 3 of 5, batch 3000 of 6980\n",
            "We are on epoch 3 of 5, batch 3500 of 6980\n",
            "We are on epoch 3 of 5, batch 4000 of 6980\n",
            "We are on epoch 3 of 5, batch 4500 of 6980\n",
            "We are on epoch 3 of 5, batch 5000 of 6980\n",
            "We are on epoch 3 of 5, batch 5500 of 6980\n",
            "We are on epoch 3 of 5, batch 6000 of 6980\n",
            "We are on epoch 3 of 5, batch 6500 of 6980\n",
            "For epoch 3 train accuracy is 85.97284549211098%, and test accuracy is 85.66491579481273%\n",
            "We are on epoch 4 of 5, batch 500 of 6980\n",
            "We are on epoch 4 of 5, batch 1000 of 6980\n",
            "We are on epoch 4 of 5, batch 1500 of 6980\n",
            "We are on epoch 4 of 5, batch 2000 of 6980\n",
            "We are on epoch 4 of 5, batch 2500 of 6980\n",
            "We are on epoch 4 of 5, batch 3000 of 6980\n",
            "We are on epoch 4 of 5, batch 3500 of 6980\n",
            "We are on epoch 4 of 5, batch 4000 of 6980\n",
            "We are on epoch 4 of 5, batch 4500 of 6980\n",
            "We are on epoch 4 of 5, batch 5000 of 6980\n",
            "We are on epoch 4 of 5, batch 5500 of 6980\n",
            "We are on epoch 4 of 5, batch 6000 of 6980\n",
            "We are on epoch 4 of 5, batch 6500 of 6980\n",
            "For epoch 4 train accuracy is 86.26642710178068%, and test accuracy is 85.91078290621803%\n",
            "We are on epoch 5 of 5, batch 500 of 6980\n",
            "We are on epoch 5 of 5, batch 1000 of 6980\n",
            "We are on epoch 5 of 5, batch 1500 of 6980\n",
            "We are on epoch 5 of 5, batch 2000 of 6980\n",
            "We are on epoch 5 of 5, batch 2500 of 6980\n",
            "We are on epoch 5 of 5, batch 3000 of 6980\n",
            "We are on epoch 5 of 5, batch 3500 of 6980\n",
            "We are on epoch 5 of 5, batch 4000 of 6980\n",
            "We are on epoch 5 of 5, batch 4500 of 6980\n",
            "We are on epoch 5 of 5, batch 5000 of 6980\n",
            "We are on epoch 5 of 5, batch 5500 of 6980\n",
            "We are on epoch 5 of 5, batch 6000 of 6980\n",
            "We are on epoch 5 of 5, batch 6500 of 6980\n",
            "For epoch 5 train accuracy is 86.47561653570835%, and test accuracy is 86.01566328241191%\n",
            "The final test accuracy of this convolutional neural network on EMNIST is 86.01566328241191%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
